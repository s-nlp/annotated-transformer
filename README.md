# The Annotated Transformer

Code for The Annotated Transformer blog post:

http://nlp.seas.harvard.edu/2018/04/03/attention.html

---

We, [SkoltechNLP](https://sites.skoltech.ru/nlp/) group, revisited this notebook in April 2021 and made some adjusments to make it runnable like "Run All Cells" with updated required packages.
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/170NpH7uVIVVTXpzjnUipUorkE2hrm4KP?usp=sharing)

Aslo, we add some clarifing images from the amazing resources about Transformers that emerged during these years:
* [Seq2Seq and Attention by Lena Voita](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html)
* [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

We believe that will add some additional understanding of the main idea and code. Enjoy your Transformer!

*   Daryna Dementieva ([@dementyeva_ds](https:///dementyeva_ds) in any social network or daryna.dementieva@skoltech.ru)
*   Anton Razzhigaev (anton.razzhigaev@skoltech.ru)
*   Alexander Panchenko (a.pnachenko@skoltech.ru)